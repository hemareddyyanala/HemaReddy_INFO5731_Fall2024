{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemareddyyanala/HemaReddy_INFO5731_Fall2024/blob/main/Yanala_Hema_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "'''\n",
        "\n",
        "1. Interesting research question: I've been a fan of music since I was 7 years old, so the first question\n",
        "    that came to my mind was: How has the popularity of songs evolved over time in the top 1000 songs\n",
        "    of all time?\n",
        "\n",
        "2. The data I require to implement this is the following:\n",
        "    I broke it down into parts, so that it would be easier to have an idea of the variables I would need in my dataset.\n",
        "    - Track Number: Position of the song in the playlist\n",
        "    - Title: Name of the song\n",
        "    - Album: Album name\n",
        "    - Date Added: Date when the song was added to the playlist\n",
        "\n",
        "3. The amount of that I data need for this is detailed information for all 1000 songs in the playlist which includes:\n",
        "    song titles and albums to identify each song uniquely; date added to track when each song was added to the playlist.\n",
        "\n",
        "4. The steps for collecting and saving data:\n",
        "    1. Data Collection: Retrieving playlist data: Using Spotify API to get song track number, titles, albums, and\n",
        "    dates added because I was using Spotify's Top 1000 songs list.\n",
        "\n",
        "    2. Data Storage: First, i'll be creating a DataFrame for data collection, I will organize the collected data into a\n",
        "    DataFrame with columns for Track Number, Title, Album, Date Added and then, saving it to CSV to export the DataFrame to a CSV file.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "9a480759-2020-45a5-9641-306dba281fc4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n1. Interesting research question: I've been a fan of music since I was 7 years old, so the first question\\n    that came to my mind was: How has the popularity of songs evolved over time in the top 1000 songs\\n    of all time?\\n\\n2. The data I require to implement this is the following:\\n    I broke it down into parts, so that it would be easier to have an idea of the variables I would need in my dataset.\\n    - Track Number: Position of the song in the playlist\\n    - Title: Name of the song\\n    - Album: Album name\\n    - Date Added: Date when the song was added to the playlist\\n\\n3. The amount of that I data need for this is detailed information for all 1000 songs in the playlist which includes:\\n    song titles and albums to identify each song uniquely; date added to track when each song was added to the playlist.\\n\\n4. The steps for collecting and saving data:\\n    1. Data Collection: Retrieving playlist data: Using Spotify API to get song track number, titles, albums, and\\n    dates added because I was using Spotify's Top 1000 songs list.\\n\\n    2. Data Storage: First, i'll be creating a DataFrame for data collection, I will organize the collected data into a\\n    DataFrame with columns for Track Number, Title, Album, Date Added and then, saving it to CSV to export the DataFrame to a CSV file.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spotipy #installing spotipy to easily interact and get data through with the Spotify Web API"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_03yQv0Nrgu",
        "outputId": "487ea4b6-7fe9-458f-8a74-dad604e544d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spotipy in /usr/local/lib/python3.10/dist-packages (2.24.0)\n",
            "Requirement already satisfied: redis>=3.5.3 in /usr/local/lib/python3.10/dist-packages (from spotipy) (5.0.8)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from spotipy) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from spotipy) (2.0.7)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis>=3.5.3->spotipy) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->spotipy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->spotipy) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->spotipy) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spotipy #importing the Spotipy library for easy with the Spotify web API\n",
        "from spotipy.oauth2 import SpotifyClientCredentials ##importing SpotifyClientCredentials from spotipy.oauth2 for handling authentication\n",
        "import pandas as pd #importing pandas for data manipulation\n",
        "\n",
        "#I created a spotify developer account for setting up the authentication\n",
        "client_id = '9dd8ac1bface4f4fbffc74d28b71d384'\n",
        "client_secret = '31ecdb60a753455695bf33a95bca6660'\n",
        "#acessing the spotify web api authentication with my credentials\n",
        "spotify = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=client_id, client_secret=client_secret))\n",
        "#URL of Top 1000 Songs plalist\n",
        "playlist_link = 'https://open.spotify.com/playlist/3Q1DIJ51dJpUO6RhnIHdVx'\n",
        "#getting the ID of the playlist by splitting the URL using split() function and taking the last segment from it\n",
        "playlist_id = playlist_link.split('/')[-1]\n",
        "#initializing an empty list to store track info\n",
        "track_details = []\n",
        "\n",
        "#I was only getting 100 samples so I used Offset and limit for pagination\n",
        "offset = 0 # offset is set to 0 so that the request will start from the beginning.\n",
        "limit = 100 # limit is set to 100 so each request will return up to 100 items.\n",
        "track_number = 1 #initialising the track number\n",
        "\n",
        "# using while to loop through the playlist and retrieving 100 tracks at a time\n",
        "while True:\n",
        "    # getting the playlist details with pagination (using the offeset and limit to determine where to begin and how many samples to retrieve)\n",
        "    playlist_data = spotify.playlist_tracks(playlist_id, offset=offset, limit=limit)\n",
        "    # using for loop to iterate through every item(track) in the playlist\n",
        "    for item in playlist_data['items']:\n",
        "        track = item['track'] #accessing the track object details\n",
        "        title = track['name'] #accessing the track title\n",
        "        album = track['album']['name'] #accessing the album name\n",
        "        date_added = item['added_at']  # accesing the date when the track was added\n",
        "        track_details.append([track_number, title, album, date_added]) #using append() function to add the track details to the empty list\n",
        "        track_number += 1 #incrementing the track number\n",
        "    # using if condition to check if the number of items received is less than the limit\n",
        "    if len(playlist_data['items']) < limit: # if yes, then there are no more tracks to fetch\n",
        "        break #exiting the loop once we reached the end of the playlist\n",
        "    #updating the offset to fetch the next page of tracks by increasing the offset by adding limit to it\n",
        "    offset += limit\n",
        "\n",
        "# Converting the track list information to DataFrame with columns as track number, title, album, date added using pandas\n",
        "dataset = pd.DataFrame(track_details, columns=['Track Number', 'Title', 'Album', 'Date added'])\n",
        "# using pandas dataframe to save the above dataframe to CSV\n",
        "dataset.to_csv('spotify_top_1000_playlist.csv', index=False)\n",
        "print(\"Data has been scraped from Spotify and stored into 'spotify_playlist.csv'\")"
      ],
      "metadata": {
        "id": "vsd17xBiDHgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ac4ba6-4bb3-4a6f-ede2-08b1ec9a4eb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been scraped from Spotify and stored into 'spotify_playlist.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "\n",
        "import requests #importing the requests library to make HTTP requests\n",
        "from bs4 import BeautifulSoup #importing beautifulsoup from bs4 to parse and extract data\n",
        "import pandas as pd #importing pandas for data manipulation\n",
        "import time #importing time to delays\n",
        "import re #importing re to operations like splitting strings\n",
        "\n",
        "# creating a function for getting article information from one search result\n",
        "def extract_article_info(result):\n",
        "    try: #using try-except to handle errors during extraction\n",
        "        tag = result.find('h3', class_='gs_rt') #extract the title element using the h3 tag\n",
        "        title = tag.a.text if tag and tag.a else \"N/A\" # extracting the title text from the tag. if tag exists, get text; or return \"N/A\"\n",
        "        url = tag.a['href'] if tag and tag.a else \"N/A\" # extracting the article url from the tag. if tag exists, get url; or return \"N/A\"\n",
        "        #extracting the data (authors, venue, and year) from 'div' tag with class 'gs_a', which has this in the result\n",
        "        tag_info = result.find('div', class_='gs_a')\n",
        "        #using if-else condition to check if the metadata exists, retrieve the text or set it to \"N/A\"\n",
        "        text_info = tag_info.text if tag_info else \"N/A\"\n",
        "        #splitting the data string using regex into parts which will separate authors, venue, and year\n",
        "        splitting = re.split('[-|—]', text_info)\n",
        "\n",
        "        #joining the remaining parts to get authors' names. It is the first part of the metadata\n",
        "        authors = ', '.join(splitting[:-2]).strip() if len(splitting) > 2 else \"N/A\"\n",
        "        #taking the venue by taking second-to-last part of the metadata, if it exists\n",
        "        venue = splitting[-2].strip() if len(splitting) > 1 else \"N/A\"\n",
        "        #getting the year from the metadata, year is the last part of the string, so we check if last part is a valid year, then strip() extra spaces\n",
        "        year = splitting[-1].strip() if splitting[-1].strip().isdigit() else \"N/A\"\n",
        "\n",
        "        #getting the abstract of the article from the 'div' tag with the class 'gs_rs'\n",
        "        abstract_tag = result.find('div', class_='gs_rs')\n",
        "        #retrieving the above text. using if-else to check If it doesn't exist, return \"N/A\".\n",
        "        abstract = abstract_tag.text.strip() if abstract_tag else \"N/A\"\n",
        "        #returning all the gathered data as a dictionary, which contains keys as Title, URL, Authors, Venue, Year, and Abstract\n",
        "        return {\"Title\": title, \"URL\": url, \"Authors\": authors, \"Venue\": venue, \"Year\": year, \"Abstract\": abstract}\n",
        "\n",
        "    except Exception as e: #handling errors that occur during the extraction and printing an error message\n",
        "        print(f\"Error extracting information: {e}\")\n",
        "        #returning default value (\"N/A\") in case an error occurs\n",
        "        return {\"Title\": \"N/A\", \"URL\": \"N/A\", \"Authors\": \"N/A\", \"Venue\": \"N/A\", \"Year\": \"N/A\", \"Abstract\": \"N/A\"}\n",
        "\n",
        "#creating a function to fetch search results from Google Scholar based on one query\n",
        "def fetch_search_results(query, start=0):  #the 'start' parameter controls pagination which will help us get different page results\n",
        "    s_url = f\"https://scholar.google.com/scholar?start={start}&q={query}&hl=en\"\n",
        "    #setting custom headers for browser request which helps avoid being blocked by Google as a bot\n",
        "    header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Apple Mac OS X 12_6_1) AppleWebKit/537.36 (KHTML, like Gecko) Version/15.6 Safari/537.36'}\n",
        "    #sending the GET() request to Google Scholar to collect the search results\n",
        "    response = requests.get(s_url, headers=header)\n",
        "    #checking if the response status code is 200. If false, we return an empty list.\n",
        "    if response.status_code != 200:\n",
        "        return []\n",
        "    #parsing the HTML content of the response using BeautifulSoup\n",
        "    b_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    #finding the search result containers on the page\n",
        "    result = b_soup.find_all('div', class_='gs_ri')\n",
        "    #returning the list of result containers\n",
        "    return result\n",
        "\n",
        "#initiating a main function to collect articles by repeatedly fetching search results\n",
        "def collect_articles(query, n_articles, start_year, end_year):\n",
        "    article_details = [] #initializing an empty list to store all the collected articles\n",
        "    start = 0  #starting with the first page of search results at index 0\n",
        "    print(\"The total number of articles collected:\", n_articles) #printing the number of articles for collecting\n",
        "    while len(article_details) < n_articles: #looping until we collect the desired number of articles\n",
        "        results = fetch_search_results(query, start) #fetching a page of search results using the fetch_search_results() function\n",
        "        if not results: #if no results are found, stop the loop\n",
        "            break\n",
        "\n",
        "        for result in results: # using for loop through each result to extract information about the article\n",
        "            articles = extract_article_info(result) #extracting article information (title, authors, year) using the extract_article_info() function\n",
        "            if articles[\"Year\"].isdigit() and start_year <= int(articles[\"Year\"]) <= end_year: #checking if the publication year is within the specified range\n",
        "                article_details.append(articles) #if the article matches the criteria, we add it to the list of all_articles\n",
        "                if len(article_details) >= n_articles: #if we have already collected the articles, break the loop\n",
        "                    break\n",
        "\n",
        "        start += 10 #incrementing the 'start' variable by 10 to move to the next page of search results\n",
        "        time.sleep(5) #Add a delay of 5 seconds between requests to avoid overloading the server\n",
        "\n",
        "    return article_details #returning the list of collected articles\n",
        "\n",
        "#defining the XYZ search query and the number of articles we want to collect\n",
        "keyword = \"XYZ\" #specifying the keyword to search for on Google Scholar\n",
        "n_articles = 1000  #the total number of articles to collect\n",
        "start = 2014 #start of the publication year range\n",
        "end = 2024 #end of the publication year range\n",
        "#calling the collect_articles() function to collect the specified number of articles\n",
        "articles = collect_articles(keyword, n_articles, start, end)\n",
        "#converting the list of collected articles into a pandas DataFrame\n",
        "dataset = pd.DataFrame(articles)\n",
        "#saving the DataFrame to a CSV file named 'XYZ_articles.csv'\n",
        "dataset.to_csv('XYZ_articles.csv', index=False)\n",
        "#printing a message to show that the data has been saved to the CSV file\n",
        "print(\"Scrapping has finished and data is saved to XYZ_articles.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX3mPQRZUV86",
        "outputId": "223e03ff-d20b-475c-810a-41dd50418a76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of articles collected: 1000\n",
            "Scrapping has finished and data is saved to XYZ_articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw #installing Python Reddit API Wrapper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds6xo1csukld",
        "outputId": "56968ce7-d4ef-4c5b-ebd9-1585471e2a43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36169097-2671-4275-b9ec-edf37a6bb4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping is finished and stored into 'reddit_posts_taylor_swift.csv'\n"
          ]
        }
      ],
      "source": [
        "import praw #importing praw as it eases the interactions with Reddit API for fetching and managing Reddit data and authentication.\n",
        "import pandas as pd #importing pandas for data manipulation\n",
        "\n",
        "#I created an app using my Reddit Developer account to get my credentials to set up Reddit API\n",
        "reddit_client = praw.Reddit(\n",
        "    client_id = 'Fmc12MZWjnf9c4QSFJnEyw', # Giving my client_id\n",
        "    client_secret = '_7Om2unZmWmzuegORLcR96SjysUvbw', # giving my client_secret\n",
        "    user_agent = 'hema_reddy' # setting up my user_agent\n",
        ")\n",
        "#defining the subreddit and keyword\n",
        "subreddit_search = 'music'  #subreddit is to search in, I searched for 'music', which is a community for music-related content\n",
        "keyword = 'taylor swift'  #keyword is to search for within posts in the above subreddit, in this case 'taylor swift' for finding posts.\n",
        "# loading the subreddit using the subreddit() method\n",
        "subreddit = reddit_client.subreddit(subreddit_search) # using the reddit.subreddit() to load the specified subreddit ('music')\n",
        "#initializing a list to store post data\n",
        "post_details = [] # this will store information about each post we collect, where post's details (e.g., title, author, score, etc.) will be appended\n",
        "#collecting posts from the subreddit containing the keyword 'taylor swift'\n",
        "for submission in subreddit.search(keyword, sort='new', limit=100): #using the search() method to search the subreddit for the keyword 'taylor swift', sorted by new first, and limits the search to 100 posts.\n",
        "    #store post details in a dictionary called 'post_data'\n",
        "    data = {\n",
        "        'Title': submission.title, #title of the post\n",
        "        'Author': submission.author.name if submission.author else 'Unknown',  #username of the post author, or 'Unknown' if the author is not there (deleted account)\n",
        "        'Date': submission.created_utc, #date of when post was created in UTC\n",
        "        'Score': submission.score,  #post's score, which is net upvotes minus downvotes\n",
        "        'Content': submission.selftext, #the content of the post (for text-based posts)\n",
        "        'URL': submission.url #direct URL of the post on Reddit\n",
        "    }\n",
        "    #appending the post information (stored as a dictionary) to the `post_details` using the append() method\n",
        "    post_details.append(data)\n",
        "#converting the post list information to DataFrame with columns as title, author, date, score, content and url added using pandas\n",
        "dataset_reddit = pd.DataFrame(post_details)\n",
        "# using pandas dataframe to save the above dataframe to CSV\n",
        "dataset_reddit.to_csv('reddit_posts_taylor_swift.csv', index=False)\n",
        "#printing a message after scraping is finished\n",
        "print(\"Scraping is finished and stored into 'reddit_posts_taylor_swift.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "\n",
        "1. Learning Experience: Describe your overall learning experience in working on web scraping tasks.\n",
        "What were the key concepts or techniques you found most beneficial in understanding the process of extracting data\n",
        "from various online sources?\n",
        "-> The learning experience was okay. I spent almost 3 days researching google and using stackoverflow for help.\n",
        "I learned a lot with this exercise more than what was taught. I learned how to use Praw API and Spotify web API, which was fun, but also\n",
        "very challenging. The exercises are very hard to do and need a lot of time to complete.\n",
        "\n",
        "2. Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them?\n",
        "If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "-> I faced a lot of challenges in completing question 3, because google scholar has a lot of restrictions. Also, using other websites\n",
        "gave me a lot of errors.\n",
        "\n",
        "3. Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work\n",
        "or research?\n",
        "-> I aspire to become a machine learning engineer, and the field requires me to work of huge amounts of data. So web scraping is fairly\n",
        "necessary to create dataset and train models based on them.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "296f5a4e-bb6a-4e76-d62d-f68184e722ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWrite your response here.\\n\\n1. Learning Experience: Describe your overall learning experience in working on web scraping tasks.\\nWhat were the key concepts or techniques you found most beneficial in understanding the process of extracting data\\nfrom various online sources?\\n-> The learning experience was okay. I spent almost 3 days researching google and using stackoverflow for help.\\nI learned a lot with this exercise more than what was taught. I learned how to use Praw API and Spotify web API, which was fun, but also\\nvery challenging. The exercises are very hard to do and need a lot of time to complete. \\n\\n2. Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them?\\nIf you opted for the non-coding option, share your experience with the chosen tool.\\n-> I faced a lot of challenges in completing question 3, because google scholar has a lot of restrictions. Also, using other websites\\ngave me a lot of errors.\\n\\n3. Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work\\nor research?\\n-> I aspire to become a machine learning engineer, and the field requires me to work of huge amounts of data. So web scraping is fairly\\nnecessary to create dataset and train models based on them.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}